# Guide d'int√©gration LLM pour l'analyse comptable

## Vue d'ensemble

Le composant `ClientsLedgerPage` int√®gre maintenant des fonctionnalit√©s d'intelligence artificielle pour faciliter l'analyse des grands livres clients et la g√©n√©ration automatique de demandes de justificatifs.

## Fonctionnalit√©s IA ajout√©es

### 1. Analyse intelligente des √©critures
- **D√©tection automatique** des factures non sold√©es
- **Identification** des paiements sans justificatifs
- **Signalement** des √©critures suspectes (montants ronds, descriptions vagues)
- **Suggestions d'am√©lioration** bas√©es sur l'analyse des patterns

### 2. G√©n√©ration automatique de messages
- **Messages personnalis√©s** pour demander des justificatifs
- **Adaptation du ton** selon le type d'√©criture (facture/paiement)
- **Contextualisation** avec les informations sp√©cifiques du client
- **Fallback intelligent** vers des templates pr√©d√©finis

### 3. Interface utilisateur am√©lior√©e
- **Panneau d'analyse** avec m√©triques visuelles
- **S√©lection en lot** des √©critures probl√©matiques
- **Indicateurs de statut** pour chaque √©criture
- **Historique des demandes** avec statut de suivi

## Options LLM disponibles (par ordre de recommandation)

### Option 1: Hugging Face Inference API (Gratuit)
```bash
# Cr√©er un compte sur https://huggingface.co/
# G√©n√©rer un token API gratuit
# Ajouter dans .env.local (ne jamais le committer) :
HUGGING_FACE_TOKEN=hf_votre_token_ici
```

> Important: Ne mettez jamais de token r√©el dans la documentation ni dans le code versionn√©. Utilisez des variables d'environnement (ex: `.env.local`, d√©j√† ignor√© par `.gitignore`). Si un token a √©t√© expos√©, r√©voquez-le imm√©diatement dans votre compte Hugging Face et g√©n√©rez-en un nouveau.

**Avantages :**
- Compl√®tement gratuit
- Pas d'installation locale requise
- Mod√®les pr√©-entra√Æn√©s disponibles

**Inconv√©nients :**
- Limitations de d√©bit
- Qualit√© variable selon les mod√®les
- D√©pendance r√©seau

### Option 2: Ollama (Gratuit, local)
```bash
# Installation sur macOS
brew install ollama

# D√©marrer le service
ollama serve

# T√©l√©charger un mod√®le (ex: Llama 2)
ollama pull llama2

# Configurer dans .env.local :
OLLAMA_BASE_URL=http://localhost:11434
```

**Avantages :**
- Compl√®tement gratuit
- Fonctionne hors ligne
- Contr√¥le total sur les donn√©es
- Mod√®les performants (Llama, Mistral, etc.)

**Inconv√©nients :**
- N√©cessite installation locale
- Consomme des ressources syst√®me
- Temps de r√©ponse plus lent

### Option 3: Serveur OpenAI‚Äëcompatible (gpt-oss, LM Studio, etc.)
Si vous disposez d‚Äôun serveur OpenAI‚Äëcompatible (par ex. gpt-oss) exposant l‚Äôendpoint `POST /v1/chat/completions`, vous pouvez le brancher sans code.

1) Variables d‚Äôenvironnement (`.env.local`)
```env
# URL de base vers votre serveur OpenAI‚Äëcompatible (ex: gpt-oss)
OPENAI_API_BASE=http://localhost:1234/v1

# Mod√®le par d√©faut √† utiliser c√¥t√© serveur
OPENAI_MODEL=gpt-oss-model-id

# Cl√© API (optionnelle si votre serveur n‚Äôen exige pas)
# OPENAI_API_KEY=sk-votre-cle-ici
```

2) Priorit√© et d√©tection
- L‚Äôapp d√©tecte d‚Äôabord Ollama (local), puis un endpoint OpenAI‚Äëcompatible (si `OPENAI_API_BASE` ou `OPENAI_API_KEY` est d√©fini), puis Hugging Face.

3) Test rapide (facultatif)
Assurez-vous que votre serveur r√©pond sur `/v1/chat/completions` au format OpenAI. Exemple de requ√™te JSON attendue¬†:
```json
{
  "model": "gpt-oss-model-id",
  "messages": [
    { "role": "system", "content": "You are a helpful assistant" },
    { "role": "user", "content": "Bonjour" }
  ]
}
```

4) Notes
- Si `OPENAI_API_KEY` n‚Äôest pas d√©fini, l‚Äôapp n‚Äôenverra pas l‚Äôent√™te Authorization.
- Personnalisez `OPENAI_MODEL` selon l‚ÄôID expos√© par votre serveur.

### Option 3: OpenAI API (Payant)
```bash
# Ajouter dans .env.local :
OPENAI_API_KEY=your-api-key-here
```

**Avantages :**
- Tr√®s haute qualit√©
- R√©ponses rapides
- API stable et document√©e

**Inconv√©nients :**
- Co√ªt par utilisation
- D√©pendance √† un service externe

## Installation et configuration

### 1. Installer les d√©pendances
```bash
cd masyzarac
npm install
```

### 2. Configurer les variables d'environnement
Choisir une des options LLM ci-dessus et configurer le fichier `.env.local`.

### 3. Tester l'int√©gration
```bash
npm run dev
```

Naviguer vers le grand livre clients et tester :
1. Importer un fichier CSV/Excel
2. Cliquer sur "Analyse IA"
3. S√©lectionner des √©critures probl√©matiques
4. G√©n√©rer des demandes de justificatifs

## Utilisation des fonctionnalit√©s

### Analyse automatique
1. **Ouvrir le grand livre clients**
2. **Cliquer sur "Analyse IA"** dans la barre d'outils
3. **Consulter les m√©triques** : factures non sold√©es, justificatifs manquants, √©critures suspectes
4. **Lire les suggestions d'am√©lioration** g√©n√©r√©es par l'IA

### G√©n√©ration de demandes
1. **S√©lectionner les √©critures** probl√©matiques (cases √† cocher)
2. **Cliquer sur "Demander justificatifs"**
3. **L'IA g√©n√®re automatiquement** des messages personnalis√©s
4. **R√©viser et envoyer** les demandes aux clients

### Actions en lot
- **S√©lectionner factures non sold√©es** : s√©lectionne automatiquement toutes les factures impay√©es
- **S√©lectionner paiements sans justif** : s√©lectionne les encaissements sans r√©f√©rences claires
- **S√©lectionner tout probl√©matique** : combine les deux cat√©gories pr√©c√©dentes

## Personnalisation des messages

Le service LLM peut √™tre personnalis√© en modifiant `src/services/llmService.ts` :

```typescript
// Modifier les prompts pour adapter le ton
private buildJustificationPrompt(context: AnalysisContext): string {
  return `G√©n√®re un message [VOTRE_STYLE] pour demander...`;
}

// Ajouter des r√®gles d'analyse sp√©cifiques
private performLocalAnalysis(description: string, amount: number) {
  // Vos r√®gles m√©tier sp√©cifiques
}
```

## S√©curit√© et confidentialit√©

### Donn√©es sensibles
- Les donn√©es comptables ne sont **jamais stock√©es** par les services LLM externes
- Seuls les **contextes n√©cessaires** sont transmis (montants, descriptions g√©n√©riques)
- Les **informations personnelles** sont automatiquement anonymis√©es

### Recommandations
1. **Utiliser Ollama** pour les donn√©es tr√®s sensibles (traitement local)
2. **Configurer des timeouts** appropri√©s pour √©viter les blocages
3. **Monitorer l'utilisation** des APIs externes pour contr√¥ler les co√ªts
4. **Tester r√©guli√®rement** les fallbacks en cas de panne des services

## D√©pannage

### Probl√®mes courants

**L'analyse ne fonctionne pas :**
- V√©rifier la configuration des variables d'environnement
- Contr√¥ler la connectivit√© r√©seau (pour APIs externes)
- Consulter les logs de la console navigateur

**Messages g√©n√©riques au lieu de personnalis√©s :**
- L'IA utilise automatiquement des templates de fallback
- V√©rifier la validit√© du token API
- Contr√¥ler les quotas d'utilisation

**Performance lente :**
- Ollama : v√©rifier les ressources syst√®me disponibles
- APIs externes : contr√¥ler la latence r√©seau
- Consid√©rer la mise en cache des r√©ponses fr√©quentes

### Logs et debugging
```javascript
// Activer les logs d√©taill√©s dans llmService.ts
console.log('ü§ñ LLM Request:', prompt);
console.log('ü§ñ LLM Response:', response);
```

## √âvolutions futures

### Fonctionnalit√©s pr√©vues
- **Analyse pr√©dictive** des risques de non-paiement
- **Classification automatique** des types d'√©critures
- **D√©tection de fraudes** bas√©e sur les patterns
- **Rapports d'analyse** automatis√©s
- **Int√©gration avec d'autres modules** (fournisseurs, immobilisations)

### Am√©liorations techniques
- **Cache intelligent** des r√©ponses LLM
- **Fine-tuning** des mod√®les sur donn√©es comptables
- **Interface de configuration** des r√®gles d'analyse
- **API REST** pour int√©grations externes

## Support

Pour toute question ou probl√®me :
1. Consulter les logs de l'application
2. V√©rifier la documentation des APIs LLM utilis√©es
3. Tester avec des donn√©es d'exemple
4. Contacter l'√©quipe de d√©veloppement avec les d√©tails de l'erreur