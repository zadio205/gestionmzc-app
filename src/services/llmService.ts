/**
 * Service pour l'int√©gration de LLM open source gratuits
 * Supporte Ollama (local), Hugging Face (gratuit), et OpenAI (payant)
 */

import { LLMProvider, AnalysisContext } from './llm/providers/LLMProvider';
import { ollamaService } from './ollamaService';
import { LLMError } from './llm/LLMError';
import { PromptSanitizer } from '@/utils/promptSanitizer';

interface LLMResponse {
  success: boolean;
  data?: string;
  error?: string;
  provider?: 'ollama' | 'huggingface' | 'openai' | 'fallback';
}

type LLMProviderType = 'ollama' | 'huggingface' | 'openai' | 'auto';

class LLMService {
  private readonly HUGGING_FACE_API = 'https://api-inference.huggingface.co/models';
  // Autoriser un endpoint OpenAI-compatible (ex: gpt-oss) via variable d'environnement
  private readonly OPENAI_API = process.env.OPENAI_API_BASE || 'https://api.openai.com/v1';
  
  // Mod√®les par provider
  private readonly MODELS = {
    huggingface: {
      text_generation: 'gpt2',
      question_answering: 'distilbert-base-cased-distilled-squad',
      text_classification: 'cardiffnlp/twitter-roberta-base-sentiment-latest'
    },
    ollama: {
      default: 'llama2',
      alternatives: ['mistral', 'codellama', 'neural-chat']
    },
    openai: {
      default: 'gpt-3.5-turbo',
      premium: 'gpt-4'
    }
  };

  private preferredProvider: LLMProviderType = 'auto';

  /**
   * D√©termine le meilleur provider disponible
   */
  private async getBestProvider(): Promise<string> {
    if (this.preferredProvider !== 'auto') {
      return this.preferredProvider;
    }

    // Priorit√© : Ollama (gratuit, local) > OpenAI (payant) > Hugging Face (gratuit, limit√©)
    
    // V√©rifier Ollama (local)
    try {
      if (await ollamaService.isAvailable()) {
        console.log('ü§ñ Utilisation d\'Ollama (local)');
        return 'ollama';
      } else {
        console.log('‚ÑπÔ∏è Ollama non disponible - Voir OLLAMA_SETUP.md pour l\'installation');
      }
    } catch (error) {
      console.log('‚ÑπÔ∏è Ollama non accessible:', error instanceof Error ? error.message : 'Erreur inconnue');
    }

    // V√©rifier OpenAI
  // Consid√©rer disponible si une cl√© est fournie OU si un endpoint custom est d√©fini
  if (process.env.OPENAI_API_KEY || process.env.OPENAI_API_BASE) {
      console.log('ü§ñ Utilisation d\'OpenAI');
      return 'openai';
    }

    // V√©rifier Hugging Face
    if (process.env.HUGGING_FACE_TOKEN) {
      console.log('ü§ñ Utilisation de Hugging Face');
      return 'huggingface';
    }

    console.log('ü§ñ Aucun LLM configur√©, utilisation des templates (mode d√©grad√©)');
    console.log('üí° Pour am√©liorer la qualit√© des messages, installez Ollama (voir OLLAMA_SETUP.md)');
    throw new LLMError('Aucun provider LLM disponible', 'none');
  }



  /**
   * G√©n√®re un message de demande de justificatif personnalis√©
   */
  async generateJustificationMessage(context: AnalysisContext): Promise<string> {
    let sanitizedContext: AnalysisContext;
    
    try {
      // Sanitize input before processing
      sanitizedContext = PromptSanitizer.sanitizeContext(context);
    } catch (sanitizationError) {
      console.error('Erreur lors de la sanitisation:', sanitizationError);
      sanitizedContext = context; // Fallback to original context
    }

    try {
      const provider = await this.getBestProvider();
      
      switch (provider) {
        case 'ollama':
          return await this.generateWithOllama(sanitizedContext);
        
        case 'openai':
          return await this.generateWithOpenAI(sanitizedContext);
        
        case 'huggingface':
          return await this.generateWithHuggingFace(sanitizedContext);
        
        default:
          return this.generateTemplateMessage(sanitizedContext);
      }
      
    } catch (error) {
      console.warn('Erreur LLM, utilisation du template par d√©faut:', error);
      // Always ensure we return a valid message
      try {
        return this.generateTemplateMessage(sanitizedContext);
      } catch (templateError) {
        console.error('Erreur critique lors de la g√©n√©ration du template:', templateError);
        return this.getEmergencyFallbackMessage(context.type);
      }
    }
  }

  /**
   * Message de secours en cas d'√©chec complet
   */
  private getEmergencyFallbackMessage(type: 'payment' | 'invoice'): string {
    return type === 'payment' 
      ? 'Bonjour, merci de nous fournir les justificatifs pour cet encaissement. Cordialement.'
      : 'Bonjour, merci de nous indiquer le statut de cette facture. Cordialement.';
  }

  /**
   * G√©n√©ration avec Ollama (local)
   */
  private async generateWithOllama(context: AnalysisContext): Promise<string> {
    try {
      return await ollamaService.generateJustificationMessage(context);
    } catch (error) {
      console.warn('Erreur Ollama:', error);
      throw error;
    }
  }

  /**
   * G√©n√©ration avec OpenAI
   */
  private async generateWithOpenAI(context: AnalysisContext): Promise<string> {
    const prompt = this.buildJustificationPrompt(context);
    const model = process.env.OPENAI_MODEL || this.MODELS.openai.default;
    
    try {
      const response = await fetch(`${this.OPENAI_API}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          // Certaines impl√©mentations OpenAI-compatibles n'exigent pas de cl√©.
          // On n'ajoute l'ent√™te que si une cl√© est disponible.
          ...(process.env.OPENAI_API_KEY ? { 'Authorization': `Bearer ${process.env.OPENAI_API_KEY}` } : {})
        },
        body: JSON.stringify({
          model,
          messages: [
            {
              role: 'system',
              content: 'Tu es un assistant comptable professionnel. G√©n√®re des messages courtois et professionnels en fran√ßais pour demander des justificatifs comptables.'
            },
            {
              role: 'user',
              content: prompt
            }
          ],
          max_tokens: 300,
          temperature: 0.7
        })
      });

      if (!response.ok) {
        throw new Error(`OpenAI API Error: ${response.status}`);
      }

      const data = await response.json();
      return data.choices[0]?.message?.content?.trim() || '';
      
    } catch (error) {
      console.error('Erreur OpenAI:', error);
      throw error;
    }
  }

  /**
   * G√©n√©ration avec Hugging Face
   */
  private async generateWithHuggingFace(context: AnalysisContext): Promise<string> {
    const prompt = this.buildJustificationPrompt(context);
    
    try {
      const response = await fetch(`${this.HUGGING_FACE_API}/${this.MODELS.huggingface.text_generation}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.HUGGING_FACE_TOKEN}`
        },
        body: JSON.stringify({
          inputs: prompt,
          parameters: {
            max_length: 200,
            temperature: 0.7,
            do_sample: true,
            return_full_text: false
          }
        })
      });

      if (!response.ok) {
        throw new Error(`Hugging Face API Error: ${response.status}`);
      }

      const data = await response.json();
      return data[0]?.generated_text?.trim() || '';
      
    } catch (error) {
      console.error('Erreur Hugging Face:', error);
      throw error;
    }
  }

  /**
   * Analyse la description d'une √©criture pour d√©tecter des anomalies
   */
  async analyzeEntryDescription(description: string, amount: number): Promise<{
    suspiciousLevel: 'low' | 'medium' | 'high';
    reasons: string[];
    suggestions: string[];
  }> {
    try {
      const provider = await this.getBestProvider();
      
      if (provider === 'ollama') {
        return await ollamaService.analyzeDescription(description, amount);
      }
      
      // Pour les autres providers, utiliser l'analyse locale pour l'instant
      return this.performLocalAnalysis(description, amount);
      
    } catch (error) {
      console.warn('Erreur analyse LLM:', error);
      return this.performLocalAnalysis(description, amount);
    }
  }

  /**
   * G√©n√®re des suggestions d'am√©lioration pour les √©critures comptables
   */
  async generateImprovementSuggestions(entries: any[]): Promise<string[]> {
    try {
      const provider = await this.getBestProvider();
      
      if (provider === 'ollama') {
        return await ollamaService.generateSuggestions(entries);
      }
      
      // Analyse des patterns locale
      const patterns = this.analyzePatterns(entries);
      return this.generateLocalSuggestions(patterns, entries.length);
      
    } catch (error) {
      console.warn('Erreur suggestions LLM:', error);
      const patterns = this.analyzePatterns(entries);
      return this.generateLocalSuggestions(patterns, entries.length);
    }
  }

  /**
   * Configure le provider pr√©f√©r√©
   */
  setPreferredProvider(provider: LLMProviderType): void {
    this.preferredProvider = provider;
  }

  /**
   * Obtient le statut des providers
   */
  async getProvidersStatus(): Promise<Record<string, boolean>> {
    const isBrowser = typeof window !== 'undefined';
    return {
      ollama: isBrowser ? false : await ollamaService.isAvailable(),
      openai: !!process.env.OPENAI_API_KEY,
      huggingface: !!process.env.HUGGING_FACE_TOKEN
    };
  }

  private buildJustificationPrompt(context: AnalysisContext): string {
    const typeText = context.type === 'payment' ? 'encaissement' : 'facture';
    
    return `G√©n√®re un message professionnel et courtois pour demander des justificatifs concernant ${typeText} suivant:
    
Client: ${context.clientName}
Montant: ${context.amount}‚Ç¨
Date: ${context.date}
Description: ${context.description}
R√©f√©rence: ${context.reference || 'non sp√©cifi√©e'}

Le message doit √™tre:
- Professionnel et courtois
- Sp√©cifique aux informations manquantes
- En fran√ßais
- Concis mais complet
- Inclure une formule de politesse

Message:`;
  }

  private generateTemplateMessage(context: AnalysisContext): string {
    const { amount, date, description, reference, type } = context;
    
    if (type === 'payment') {
      return `Bonjour,

Nous avons identifi√© un encaissement de ${amount}‚Ç¨ en date du ${date}.

Description: ${description}
R√©f√©rence: ${reference || 'non sp√©cifi√©e'}

Afin de compl√©ter notre suivi comptable, pourriez-vous nous fournir les justificatifs suivants :
- Copie du ch√®que ou relev√© bancaire
- Facture correspondante si applicable
- Tout autre document justificatif

Cette demande s'inscrit dans le cadre de nos obligations de contr√¥le et de tra√ßabilit√© comptable.

Merci de votre collaboration.

Cordialement,
L'√©quipe comptable`;
    } else {
      return `Bonjour,

Nous constatons que la facture d'un montant de ${amount}‚Ç¨ √©mise le ${date} pr√©sente un solde non sold√©.

Description: ${description}
R√©f√©rence: ${reference || 'non sp√©cifi√©e'}

Pourriez-vous nous indiquer :
- Le statut actuel de cette facture
- La date de r√®glement pr√©vue
- Tout √©l√©ment explicatif concernant ce dossier

Nous restons √† votre disposition pour tout compl√©ment d'information.

Cordialement,
L'√©quipe comptable`;
    }
  }

  private performLocalAnalysis(description: string, amount: number) {
    const reasons: string[] = [];
    let suspiciousLevel: 'low' | 'medium' | 'high' = 'low';

    // V√©rifications basiques
    if (description.length < 10) {
      reasons.push('Description trop courte');
      suspiciousLevel = 'medium';
    }

    if (amount % 100 === 0 && amount > 500) {
      reasons.push('Montant rond suspect');
      if (suspiciousLevel === 'low') {
        suspiciousLevel = 'medium';
      }
    }

    const suspiciousWords = ['divers', 'autre', 'inconnu', 'test'];
    if (suspiciousWords.some(word => description.toLowerCase().includes(word))) {
      reasons.push('Description vague ou g√©n√©rique');
      suspiciousLevel = 'high';
    }

    // V√©rifications avanc√©es
    if (description.match(/^\d+$/)) {
      reasons.push('Description uniquement num√©rique');
      suspiciousLevel = 'high';
    }

    if (description.length > 100) {
      reasons.push('Description anormalement longue');
      suspiciousLevel = 'medium';
    }

    const suggestions: string[] = [];
    if (reasons.length > 0) {
      suggestions.push('Demander des justificatifs compl√©mentaires');
      suggestions.push('V√©rifier la coh√©rence avec les pi√®ces comptables');
      
      if (suspiciousLevel === 'high') {
        suggestions.push('Effectuer un contr√¥le approfondi');
      }
    }

    return {
      suspiciousLevel,
      reasons,
      suggestions
    };
  }

  private analyzePatterns(entries: any[]) {
    return {
      missingReferences: entries.filter(e => !e.reference || e.reference.length < 3).length,
      vagueDescriptions: entries.filter(e => e.description.length < 15).length,
      roundAmounts: entries.filter(e => (e.debit + e.credit) % 100 === 0).length,
      weekendEntries: entries.filter(e => {
        if (!e.date) return false;
        const day = new Date(e.date).getDay();
        return day === 0 || day === 6;
      }).length,
      duplicateDescriptions: this.findDuplicateDescriptions(entries),
      highAmounts: entries.filter(e => (e.debit + e.credit) > 10000).length
    };
  }

  private findDuplicateDescriptions(entries: any[]): number {
    const descriptions = entries.map(e => e.description.toLowerCase());
    const duplicates = descriptions.filter((desc, index) => 
      descriptions.indexOf(desc) !== index
    );
    return duplicates.length;
  }

  private generateLocalSuggestions(patterns: any, totalEntries: number): string[] {
    const suggestions: string[] = [];
    
    if (patterns.missingReferences > totalEntries * 0.3) {
      suggestions.push("Am√©liorer la tra√ßabilit√© en ajoutant des r√©f√©rences syst√©matiques");
    }
    
    if (patterns.vagueDescriptions > totalEntries * 0.2) {
      suggestions.push("Enrichir les descriptions des √©critures pour faciliter le suivi");
    }
    
    if (patterns.roundAmounts > totalEntries * 0.5) {
      suggestions.push("V√©rifier les montants ronds qui peuvent indiquer des estimations");
    }

    if (patterns.weekendEntries > 0) {
      suggestions.push("Contr√¥ler les √©critures saisies en week-end");
    }

    if (patterns.duplicateDescriptions > totalEntries * 0.1) {
      suggestions.push("Diversifier les descriptions pour √©viter les doublons");
    }

    if (patterns.highAmounts > 0) {
      suggestions.push("Renforcer les contr√¥les sur les montants √©lev√©s");
    }

    // Suggestions g√©n√©rales si peu de probl√®mes d√©tect√©s
    if (suggestions.length === 0) {
      suggestions.push("Maintenir la qualit√© actuelle de la saisie");
      suggestions.push("Effectuer des contr√¥les p√©riodiques de coh√©rence");
    }
    
    return suggestions;
  }
}

// Instance singleton
export const llmService = new LLMService();

// Types pour l'export
export type { AnalysisContext, LLMResponse, LLMProvider };